{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0288fdc-1fbb-488a-98fd-c037bec05c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "from scipy.linalg import svd\n",
    "from scipy.optimize import minimize\n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee619e6-cba5-4cc9-82b7-a4b29584be2c",
   "metadata": {},
   "source": [
    "In this project done for an undergraduate DRP (Directed Reading Program), we analyze the trading strategy described by the paper \"Dynamic Mode Decomposition for Financial Trading Strategies\" by Jordan Mann and J. Nathan Kutz (2015). The strategy within the paper is based mathematically in the theory of Koopman operators from nonlinear dynamics: the core intuitive idea is to linearly approximate the next state of a given nonlinear dynamical system at a given point in time. In theory, this is accomplished by considering the composition (or Koopman) operators $K_t(g) = g\\circ F_t$, where $F_t$ is our nonlinear dynamical system, and $g$ is a real-valued function. The functions $g$ model our measurements in practice: if we take snapshots of our data at certain points in time with a discrete time-step $\\Delta t$ and $x_k$ are our real sample points, we have $K_{\\Delta t}(g(x_k)) = g(F_{\\Delta t}(x_k)) = g(x_{k+1})$. In other words, this gives us a linear approximation of the next state in our dynamical system; in practice, we take $g$ to be the Identity, and simply try to approximate the next sample points from given data in a linear fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf4ba7-d3be-4787-8b0a-7539bbcf53f3",
   "metadata": {},
   "source": [
    "The mathematical theory is applied in practice as the following problem: given a set of time series data, how can we predict future values? The idea of the paper by Mann and Kutz is to use the Koopman operator formalism and apply it to time series. That is, we now sample pairs of past data ${(x(t_k), x(t_k'))}_{k=1}^m$ where $t_k' = t_k + \\Delta t$ for some sufficiently small timestep $\\Delta t$. For example, we could consider the stock prices of home construction companies in the past 100 days, and consider data from times -80 to -20, and the incremented data from times -79 to -19. The pairs are arranged into two matrices, $X$ (past) and $X'$ (future), and one then uses best-fit linear regression to find a matrix $A$ that most closely approximates $X' \\approx AX$. The predictive value for the next time-step is then given by $Ax_{k} \\approx x_{k+1}$. In code, this is implemented by repeatedly applying linear regression on a sliding window of past data to predict future values, and then updating the matrix $A$ with each new set of future training data. Mathematically, the matrix $A$ is meant to model our Koopman operator discussed previously, and $A$ is precisely defined as $A = \\text{argmin}_A||X'-AX||_F = X'X^\\dagger$, where $||\\cdot||_F$ denotes the Frobenius norm, and $X^\\dagger$ represents the pseudo-inverse of the data matrix $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1198794-9119-4326-8724-52b6ced886f2",
   "metadata": {},
   "source": [
    "This usage of the Koopman operator theory to approximate the next time series value using past data is called the dynamic mode decomposition (DMD). To make the algorithm more efficient in computing the matrix $A$ and finding future values, one uses dimensionality reduction (e.g., PCA) to compute the dominant eigenvalues and eigenvectors of $A$ without computing it explicitly. In particular, the pseudo-inverse $X^\\dagger$ is computed via the singular value decomposition of $X$. Since $X^\\dagger$ typically has far fewer columns than rows, i.e. $m << n$, there are at most $m$ non-zero singular values and corresponding singular vectors, so the matrix $A$ will have at most rank m.\n",
    "Instead of computing $A$ directly, one then computes the projection of $A$ onto these leading singular vectors, resulting in a small matrix $\\tilde{A}$ of size at most $m \\times m$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf954896-ea01-4426-ae97-a402c5ce624b",
   "metadata": {},
   "source": [
    "Thus the full DMD algorithm is as follows: \n",
    "\n",
    "Step 1: compute the singular value decomposition of the data matrix $X = U\\Sigma V^*$\n",
    "\\\n",
    "Step 2: compute the pseudo-inverse $X^\\dagger = V\\Sigma^{-1}U^*$ to compute $A = X'X^\\dagger$. Project $A$ to the $r$ leading eigenspaces, where $r$ is a hyperparameter\n",
    "\\\n",
    "Step 3: apply $A$ to the $k$-th sample $x_k$ to obtain a prediction $Ax_k = \\hat{x}_{k+1}$ for $x_{k+1}$. Repeat with the predictive value used as a feature in a new data matrix $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6515d82-3410-407c-bbb2-de450f788e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  9 of 9 completed\n"
     ]
    }
   ],
   "source": [
    "home_construction_portfolio = ['DHI', 'LEN', 'PHM', 'TOL', 'NVR', 'HD', 'LOW', 'SHW', 'SPY']\n",
    "start_date = dt.datetime.today()-dt.timedelta(days = 100)\n",
    "end_date = dt.datetime.today()\n",
    "\n",
    "home_construction_data = yf.download(home_construction_portfolio, start = start_date, end = end_date)\n",
    "\n",
    "daily_returns = np.log(home_construction_data['Close']/home_construction_data['Close'].shift(1))\n",
    "daily_returns = daily_returns.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7249edf-5284-495a-a68c-8d778bf97f0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20a11332-9caf-41ae-9922-13dec9bc43ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticker           DHI        HD       LEN       LOW       NVR       PHM  \\\n",
      "Date                                                                     \n",
      "2025-05-28 -0.034248 -0.006337 -0.026626 -0.005729 -0.029632 -0.028850   \n",
      "2025-05-29  0.007468  0.000625  0.007761  0.001824  0.013835  0.008459   \n",
      "2025-05-30 -0.001777  0.000000  0.000094  0.003550  0.002574 -0.005189   \n",
      "2025-06-02 -0.009960 -0.000896 -0.010043 -0.000931 -0.010244 -0.004908   \n",
      "2025-06-03  0.012413  0.013819  0.015402  0.014439  0.003618  0.011416   \n",
      "...              ...       ...       ...       ...       ...       ...   \n",
      "2025-08-25 -0.010128 -0.009468 -0.007023 -0.018600 -0.010539 -0.000378   \n",
      "2025-08-26 -0.003498 -0.003332 -0.011415 -0.002088 -0.007220 -0.008732   \n",
      "2025-08-27 -0.005181  0.001716  0.000450  0.001315 -0.004276 -0.000763   \n",
      "2025-08-28  0.008383 -0.001937 -0.001501 -0.004378  0.005088  0.003352   \n",
      "2025-08-29  0.003369 -0.001670  0.000075  0.002017  0.005293  0.004175   \n",
      "\n",
      "Ticker           SHW       SPY       TOL  \n",
      "Date                                      \n",
      "2025-05-28 -0.012856 -0.005802 -0.035363  \n",
      "2025-05-29  0.002871  0.003940  0.008316  \n",
      "2025-05-30  0.008565 -0.001119 -0.007645  \n",
      "2025-06-02 -0.009437  0.005617 -0.009058  \n",
      "2025-06-03  0.006506  0.005686  0.020787  \n",
      "...              ...       ...       ...  \n",
      "2025-08-25 -0.016029 -0.004411 -0.004032  \n",
      "2025-08-26 -0.001472  0.004178 -0.006587  \n",
      "2025-08-27  0.002562  0.002276  0.001741  \n",
      "2025-08-28 -0.003326  0.003535  0.006791  \n",
      "2025-08-29 -0.000956 -0.005982  0.000864  \n",
      "\n",
      "[66 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(daily_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2e4e1584-a585-45b9-9a17-8c549550bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(M1, M2, l):\n",
    "    ''' \n",
    "    Uses Koopman related DMD algorithm to predict future prices.\n",
    "    l is the number of days in the future to predict\n",
    "    This uses the pseudo-inverse to compute A\n",
    "    ''' \n",
    "    U, s, Vt = np.linalg.svd(M1, full_matrices=False)\n",
    "    Sigma_inv = np.diag(1.0/s)\n",
    "    A_tilde = U.T @ M2 @ Vt.T @ Sigma_inv\n",
    "    evals, evecs = np.linalg.eig(A_tilde)\n",
    "    Phi = U @ evecs\n",
    "    x0 = M1[:, 0].reshape((-1,1))\n",
    "    b = np.linalg.pinv(Phi) @ x0\n",
    "\n",
    "    n, r = Phi.shape\n",
    "    preds = np.zeros((n, l))\n",
    "    for t in range(1, l+1):\n",
    "        D = np.diag(np.exp(np.log(evals) * t))\n",
    "        x_t = Phi @ D @ b\n",
    "        preds[:, t-1] = x_t.ravel().real\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4cfc23f2-b267-4d7a-b842-c8a5ee735b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_accuracy(prediction, actual):\n",
    "    '''\n",
    "    Returns the accuracy of the predictions. The model is accurate if\n",
    "    the predicted direction of the price change matches the actual direction.\n",
    "    '''\n",
    "    if prediction.shape != actual.shape:\n",
    "        raise ValueError(\"Prediction and actual arrays must have the same shape.\")\n",
    "\n",
    "    diff_pred = prediction[:, 1:] - prediction[:, :-1]  \n",
    "    diff_act = actual[:, 1:] - actual[:, :-1]     \n",
    "    \n",
    "    correct = np.sum(diff_pred * diff_act > 0)\n",
    "    return correct/diff_pred.size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fc48df2-42b8-4081-958f-2c84e667124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hotspot(data_arr):\n",
    "    ''' \n",
    "    Finds potential hotspots based on the directional accuracy of price predictions. \n",
    "    Uses past 100 days of data to run through a range of parameters (m, l) to find\n",
    "    combinations that yield an accuracy greater than 0.53.\n",
    "\n",
    "    Returns a list of tuples (m, l, accuracy) for all combinations tested.\n",
    "    '''\n",
    "    accuracy_scores = []\n",
    "    potential_hotspots = []\n",
    "\n",
    "    for m in range(4, 15):\n",
    "        for l in range(2, 10):\n",
    "            scores = []\n",
    "            for t in range(m, len(data_arr)-l): \n",
    "                M = data_arr[:, t-m:t]\n",
    "                M1 = M[:, :-1]\n",
    "                M2 = M[:, 1:]\n",
    "                preds = predict_price(M1, M2, l)[: -1, :] #drop the mean column\n",
    "                actual = data_arr[:, t:t+l][: -1, :]\n",
    "                scores.append(directional_accuracy(preds, actual))\n",
    "            accuracy = np.sum(scores) / len(scores)\n",
    "            accuracy_scores.append((m, l, accuracy))\n",
    "            if accuracy > 0.53: #arbitrary threshold\n",
    "                potential_hotspots.append((m, l, accuracy))\n",
    "    return accuracy_scores, potential_hotspots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba130a-babb-4a54-8f0d-606a1a09eacf",
   "metadata": {},
   "source": [
    "We compare 3 strategies based on their P&L:\n",
    "\n",
    "Strategy 1: split up 10 years of data into 100 day increments. Then find a hotspot using the first 100 days, invest l future days; recalibrate the hotspot after l days, and repeat on 10 years of data.\n",
    "\\\n",
    "Strategy 2: Find a hotspot over 2 years of data, then use that hotspot for the next 6 months or something. Then recalibrate.\n",
    "\\\n",
    "Strategy 3: Find a hotspot over 3 years, and use the same hotspot for the next 7 years.\n",
    "Compare all these to the benchmark S&P 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "902f053e-ef45-4c12-a404-7664e1759ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_1(portfolio):\n",
    "    '''\n",
    "    Strategy 1: split up 10 years of data into 100 day increments.\n",
    "    Then find a hotspot using the first 100 days, invest l future days; \n",
    "    recalibrate the hotspot after l days, and repeat on 10 years of data.\n",
    "    '''\n",
    "    hist_data_10_yrs = get_close_prices(portfolio)\n",
    "    n = hist_data_10_yrs.shape[1]\n",
    "    hist_data_10_yrs = hist_data_10_yrs[:, n//2:] # Use only the most recent 10 years of data\n",
    "\n",
    "    delta = 0\n",
    "    while delta < hist_data_10_yrs.shape[1]-100: #can adjust this 100 to change the number of days used to find hotspots\n",
    "        data_arr = hist_data_10_yrs[:, delta:delta+100]\n",
    "        accuracy_scores, potential_hotspots = find_hotspot(data_arr)\n",
    "        if potential_hotspots:\n",
    "            sorted_by_score = sorted(potential_hotspots, key=lambda tpl: tpl[2], reverse=True)\n",
    "            #here we can build and call a function that checks if the average of its neighbors are also > 51% \n",
    "            #for now will just take the highest accuracy score and use the corresponding (m,l) as the hotspot\n",
    "            m, l, accuracy = sorted_by_score[0]\n",
    "            #build and call a function that invests and tracks P&L using (m,l) hotspot\n",
    "            delta += l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f967ac09-78d4-4514-8340-04848f4f2894",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = sorted({m for m, l, score in accuracy_scores})\n",
    "ls = sorted({l for m, l, score in accuracy_scores})\n",
    "\n",
    "heatmap = np.zeros((len(ls), len(ms)))\n",
    "\n",
    "for (m_val, l_val, score) in accuracy_scores:\n",
    "    i = ls.index(l_val)   \n",
    "    j = ms.index(m_val) \n",
    "    heatmap[i, j] = score\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(heatmap, origin='lower', aspect='auto')\n",
    "\n",
    "ax.set_xticks(np.arange(len(ms)))\n",
    "ax.set_xticklabels(ms)\n",
    "ax.set_yticks(np.arange(len(ls)))\n",
    "ax.set_yticklabels(ls)\n",
    "\n",
    "ax.set_xticks(np.arange(len(ms) + 1) - 0.5, minor=True)\n",
    "ax.set_yticks(np.arange(len(ls) + 1) - 0.5, minor=True)\n",
    "\n",
    "ax.grid(which='minor', color='white', linestyle='-', linewidth=1)\n",
    "\n",
    "ax.tick_params(which='minor', bottom=False, left=False)\n",
    "\n",
    "ax.set_xlabel('m (look-back window)')\n",
    "ax.set_ylabel('l (prediction horizon)')\n",
    "ax.set_title('Accuracy Heatmap')\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax)\n",
    "cbar.set_label('Directional Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
